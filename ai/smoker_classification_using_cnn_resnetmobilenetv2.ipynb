{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:12.133122Z",
          "iopub.status.busy": "2023-12-25T15:33:12.132587Z",
          "iopub.status.idle": "2023-12-25T15:33:13.332185Z",
          "shell.execute_reply": "2023-12-25T15:33:13.331068Z",
          "shell.execute_reply.started": "2023-12-25T15:33:12.133079Z"
        },
        "id": "lEzJXgjDf5y8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import Data Science Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import cv2\n",
        "\n",
        "# Tensorflow Libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,models\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# System libraries\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:13.335240Z",
          "iopub.status.busy": "2023-12-25T15:33:13.334728Z",
          "iopub.status.idle": "2023-12-25T15:33:13.386177Z",
          "shell.execute_reply": "2023-12-25T15:33:13.384996Z",
          "shell.execute_reply.started": "2023-12-25T15:33:13.335190Z"
        },
        "id": "k_uIBb56nlrZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### We create a bunch of helpful functions throughout the course.\n",
        "### Storing them here so they're easily accessible.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's\n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "# Plot the validation and training data separately\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "\n",
        "  Args:\n",
        "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "  \"\"\"\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();\n",
        "\n",
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "\n",
        "    Args:\n",
        "      original_history: History object from original model (before new_history)\n",
        "      new_history: History object from continued model training (after original_history)\n",
        "      initial_epochs: Number of epochs in original_history (new_history plot starts from here)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "\n",
        "  Args:\n",
        "    dir_path (str): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "      y_true: true labels in the form of a 1D array\n",
        "      y_pred: predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:13.389245Z",
          "iopub.status.busy": "2023-12-25T15:33:13.387418Z",
          "iopub.status.idle": "2023-12-25T15:33:13.411866Z",
          "shell.execute_reply": "2023-12-25T15:33:13.410377Z",
          "shell.execute_reply.started": "2023-12-25T15:33:13.389172Z"
        },
        "id": "F8ReVC2MiBRZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# # Import series of helper functions for our notebook\n",
        "# from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyXyH3kfn5Et"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTbvsC_Nnw4t"
      },
      "outputs": [],
      "source": [
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d vitaminc/cigarette-smoker-detection\n",
        "# !unzip /content/cigarette-smoker-detection.zip -d /content\n",
        "# !rm /content/cigarette-smoker-detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:13.427551Z",
          "iopub.status.busy": "2023-12-25T15:33:13.426912Z",
          "iopub.status.idle": "2023-12-25T15:33:18.890680Z",
          "shell.execute_reply": "2023-12-25T15:33:18.889040Z",
          "shell.execute_reply.started": "2023-12-25T15:33:13.427495Z"
        },
        "id": "5kXkjadNxsNI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Walk through each directory\n",
        "dataset = \"/content/data/\"\n",
        "walk_through_dir(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:18.893793Z",
          "iopub.status.busy": "2023-12-25T15:33:18.892740Z",
          "iopub.status.idle": "2023-12-25T15:33:19.827425Z",
          "shell.execute_reply": "2023-12-25T15:33:19.826128Z",
          "shell.execute_reply.started": "2023-12-25T15:33:18.893748Z"
        },
        "id": "s14XOEp01m_s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_dir = Path(dataset)\n",
        "\n",
        "# Get filepaths and labels\n",
        "filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png'))  + list(image_dir.glob(r'**/*.jpeg')) + list(image_dir.glob(r'**/*.gif'))\n",
        "\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "image_df = pd.concat([filepaths, labels], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:19.829572Z",
          "iopub.status.busy": "2023-12-25T15:33:19.829192Z",
          "iopub.status.idle": "2023-12-25T15:33:19.858020Z",
          "shell.execute_reply": "2023-12-25T15:33:19.856985Z",
          "shell.execute_reply.started": "2023-12-25T15:33:19.829535Z"
        },
        "id": "d3-uoP4n1oqK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:32.067444Z",
          "iopub.status.busy": "2023-12-25T15:33:32.067086Z",
          "iopub.status.idle": "2023-12-25T15:33:32.078545Z",
          "shell.execute_reply": "2023-12-25T15:33:32.077135Z",
          "shell.execute_reply.started": "2023-12-25T15:33:32.067410Z"
        },
        "id": "xaJlHTlz2K4M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Separate in train and test data\n",
        "train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:32.081430Z",
          "iopub.status.busy": "2023-12-25T15:33:32.080405Z",
          "iopub.status.idle": "2023-12-25T15:33:32.088127Z",
          "shell.execute_reply": "2023-12-25T15:33:32.086699Z",
          "shell.execute_reply.started": "2023-12-25T15:33:32.081387Z"
        },
        "id": "3puUVDwl2Mcz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:32.090578Z",
          "iopub.status.busy": "2023-12-25T15:33:32.090145Z",
          "iopub.status.idle": "2023-12-25T15:33:35.544538Z",
          "shell.execute_reply": "2023-12-25T15:33:35.543412Z",
          "shell.execute_reply.started": "2023-12-25T15:33:32.090538Z"
        },
        "id": "CsftNShQ2PaK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Split the data into three categories.\n",
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2023-12-25T15:33:35.547088Z",
          "iopub.status.busy": "2023-12-25T15:33:35.546197Z",
          "iopub.status.idle": "2023-12-25T15:33:35.632141Z",
          "shell.execute_reply": "2023-12-25T15:33:35.631138Z",
          "shell.execute_reply.started": "2023-12-25T15:33:35.547041Z"
        },
        "id": "sLbR4WtD2RPg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Resize Layer\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(224,224),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "  layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "  layers.experimental.preprocessing.RandomContrast(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSEhK2w02Uk-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-25T15:34:40.027625Z",
          "iopub.status.busy": "2023-12-25T15:34:40.027119Z",
          "iopub.status.idle": "2023-12-25T15:35:05.346564Z",
          "shell.execute_reply": "2023-12-25T15:35:05.344711Z",
          "shell.execute_reply.started": "2023-12-25T15:34:40.027583Z"
        },
        "id": "z4VI_UxV2Wp2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load the pretained model\n",
        "pretrained_model = tf.keras.applications.InceptionResNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "pretrained_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.128473Z",
          "iopub.status.idle": "2023-12-25T15:33:57.129393Z",
          "shell.execute_reply": "2023-12-25T15:33:57.129105Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.129074Z"
        },
        "id": "1xn6j_La2Y2u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create checkpoint callback\n",
        "checkpoint_path = \"smokers_classification_model_checkpoint\"\n",
        "checkpoint_callback = ModelCheckpoint(checkpoint_path,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor=\"val_accuracy\",\n",
        "                                      save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWxeUam01kFc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.131206Z",
          "iopub.status.idle": "2023-12-25T15:33:57.132139Z",
          "shell.execute_reply": "2023-12-25T15:33:57.131816Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.131784Z"
        },
        "id": "YbP7g6Xh2abB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 5 epochs\n",
        "early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
        "                               patience = 5,\n",
        "                               restore_best_weights = True) # if val loss decreases for 5 epochs in a row, stop training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.133846Z",
          "iopub.status.idle": "2023-12-25T15:33:57.134719Z",
          "shell.execute_reply": "2023-12-25T15:33:57.134429Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.134398Z"
        },
        "id": "AkcAsl5H2tYl",
        "outputId": "430a3802-dd54-4ed8-98c4-bbb7f2c74081",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: training_logs/smoker_classification/20231226-022431\n",
            "Epoch 1/5\n",
            "  6/130 [>.............................] - ETA: 18:49 - loss: 2.5240 - accuracy: 0.5417"
          ]
        }
      ],
      "source": [
        "inputs = pretrained_model.input\n",
        "x = resize_and_rescale(inputs)\n",
        "\n",
        "x = Dense(256, activation='relu')(pretrained_model.output)\n",
        "x = Dropout(0.45)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.45)(x)\n",
        "\n",
        "\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(0.00001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    steps_per_epoch=len(train_images),\n",
        "    validation_data=val_images,\n",
        "    validation_steps=len(val_images),\n",
        "    epochs=5,\n",
        "    callbacks=[\n",
        "        early_stopping,\n",
        "        create_tensorboard_callback(\"training_logs\",\n",
        "                                    \"smoker_classification\"),\n",
        "        checkpoint_callback,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v6Za1j53RoU"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BWrofxS2vO0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.136163Z",
          "iopub.status.idle": "2023-12-25T15:33:57.137592Z",
          "shell.execute_reply": "2023-12-25T15:33:57.137278Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.137242Z"
        },
        "id": "CS-g90hJ340B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5oGHvsG368q"
      },
      "source": [
        "## Visualizing loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.139931Z",
          "iopub.status.idle": "2023-12-25T15:33:57.140576Z",
          "shell.execute_reply": "2023-12-25T15:33:57.140281Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.140247Z"
        },
        "id": "01SS6RVx38o7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BL7xgPz4Fv-"
      },
      "source": [
        "## Making predictions on the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.143164Z",
          "iopub.status.idle": "2023-12-25T15:33:57.144088Z",
          "shell.execute_reply": "2023-12-25T15:33:57.143857Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.143806Z"
        },
        "id": "KxAegJBB4HlW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Predict the label of the test_images\n",
        "pred = model.predict(test_images)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "\n",
        "# Map the label\n",
        "labels = (train_images.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "pred = [labels[k] for k in pred]\n",
        "\n",
        "# Display the result\n",
        "print(f'The first 5 predictions: {pred[:5]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-25T15:33:57.146013Z",
          "iopub.status.idle": "2023-12-25T15:33:57.146537Z",
          "shell.execute_reply": "2023-12-25T15:33:57.146334Z",
          "shell.execute_reply.started": "2023-12-25T15:33:57.146309Z"
        },
        "id": "pWO4e4wb4Iln",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Display 25 random pictures from the dataset with their labels\n",
        "random_index = np.random.randint(0, len(test_df) - 1, 15)\n",
        "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),\n",
        "                        subplot_kw={'xticks': [], 'yticks': []})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(plt.imread(test_df.Filepath.iloc[random_index[i]]))\n",
        "    if test_df.Label.iloc[random_index[i]] == pred[random_index[i]]:\n",
        "        color = \"green\"\n",
        "    else:\n",
        "        color = \"red\"\n",
        "    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\", color=color)\n",
        "plt.show()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_image(image_content: bytes) -> bool:\n",
        "    \"\"\"\n",
        "    Classify the image content to determine if smoking is detected.\n",
        "\n",
        "    Args:\n",
        "        image_content (bytes): The binary content of the image.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if smoking is detected, False otherwise.\n",
        "    \"\"\"\n",
        "    image_content = prep_image()\n",
        "    is_smoking = model.predict(image_content)\n",
        "    return is_smoking\n",
        "\n",
        "def prep_image(image_content, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from file_content, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filecontent(byte)): file content\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  # Decode it into a tensor\n",
        "  img = tf.image.decode_jpeg(image_content)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classify_image(\"smoking.webp\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 188204,
          "sourceId": 420647,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30235,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
